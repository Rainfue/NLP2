Искусственная нейронная сеть (ИНС) — математическая модель, а
также её программное или аппаратное воплощение, построенная по принципу
организации и функционирования биологических нейронных сетей — сетей
нервных клеток живого организма. Нейросеть состоит из нейронов
(перцептроны), связями между ними (синапсы) и нелинейной функцией
активации. На вход перцептрона поступает вектор вещественных чисел,
который перемножается на вектор весов, результат суммируется и идёт на вход
функции активации.
Перцептрон - математическая или компьютерная модель восприятия
информации мозгом. Перцептрон стал одной из первых моделей нейросетей.
Пер-цептрон состоит из трёх типов элементов, а именно: поступающие от
датчиков сигналы передаются ассоциативным элементам, а затем
реагирующим элементам.
Функции активации — нелинейности, вводимые в нейронную сеть для
того, чтобы уловить сложные взаимосвязи данных. Нелинейная функция -
функция, которую нельзя выразить в линейном виде: у=ах+К Она может быть
нелинейной в том смысле, что обладает непрерывной кривизной.
Многослойные перцептроны считаются одними из основных
стандартных блоков нейронных сетей. Простейший многослойный
перцептрон представляет собой расширение понятия перцептрона.
Перцептрон принимает на входе вектор данных и вычисляет одно
выходное значение. В многослойных нейросетях с перцептронами (MLP сети)
несколько перцептронов группируются таким образом, что выходной
результат отдельного слоя представляет собой новый вектор, а не просто одно
значение. Во фреймворке PyTorch для этого достаточно указать число
выходных признаков в слое Linear. Дополнительная особенность MLP сетей
заключается в том, что между каждыми двумя слоями вставляется
нелинейность. Код демонстрирующий создание нейросети на основе
перцептрона, ее обучение выполнению операции XOR с использованием
import torch
from torch.autograd import Variable
import torch.nn as nn import
torch.nn.functional as F import
torch.optim as optim
EPOCHS_TO_TRAIN = 15000
#50000 class Net(nn.Module):
def__init_ (self):
super(Net, self). init_ ()
self.fc1 = nn.Linear(2, 3,
True) self.fc2 = nn.Linear(3, 1,
True) def forward(self, x):
x = F.sigmoid(self.fc1 (x))
x = self.fc2(x) return x net = Net
()
inputs = list(map(lambda s: Variable(torch.Tensor
([s])), [ [0, 0],
[0, 1],
[1, 0],
[1, 1]
]))
targets = list(map(lambda s: Variable(torch.Tensor
([s])), [ [0],
[1],
[1],
[0]
]))
criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)
print("Training loop:") for idx in range(0,
EPOCHS_TO_TRAIN): for input, target in zip
(inputs, targets):
optimizer.zero_grad() # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
173
loss.backward()
optimizer.step() # Does the update if idx % 5000 == 0:
print("Epoch {: >8} Loss: {}".format(idx, loss.data.numpy
())) print("")
print("Final results:") for input, target in zip
(inputs, targets): output = net(input)
print("Input:[{},{}] Target:[{}] Predicted:[{}] Error:
[{}]".format( int(input.data.numpy() [0] [0]), int
(input.data.numpy() [0] [1]), int(target.data.numpy()[0]),
round(float(output.data.numpy()[0]), 4),
round(float(abs(target.data.numpy()[0] - output.data.numpy())), 4)
))
Объекты Linear называются здесь fc1 и fc2 по общепринятому
соглашению, именующему модуль Linear полносвязным слоем (fully connected
layer, или для краткости fc layer) 1 . Помимо этих двух линейных слоев, в нем
присутствует нелинейность — выпрямленный линейный блок, он
применяется к выходным результатам первого линейного слоя до отправки их
на вход второму линейному слою. По причине последовательной сущности
слоев необходимо убедиться в том, что количество выходных значений слоя
равно количеству входных значений следующего слоя.
Таким образом разработанные учебные материалы для изучения
перцеп-трона с использованием библиотеки PyTorch могут использоваться для
проведения практических работ студентами, обучающимися дисциплине
«Интеллектуальные системы управления».