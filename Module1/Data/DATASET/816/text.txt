В настоящие время искусственный интеллект все чаще и чаще 
применяется для различных задач, начиная от «умной колонки» и заканчивая 
автопилотом в автомобилях. Искусственному интеллекту посвящено множество работ, 
среди которых, в частности, и работы, посвященные нейросетевым алгоритмам 
распознавания, анализа и классификации образов [1, 2], дополненной реальности [3]. В 
2018 году, компания NVIDIA представила видеокарты на микроархитектуре Turing с 
тензорными ядрами, которые использовали результат глубокого обучения нейросети 
тем самым, улучшая качество изображения в играх или приложениях. Данный способ 
компания назвала Deep learning super sampling или просто DLSS. В противовес 
компании NVIDIA, компания AMD выпустила свою технологию улучшения качества 
изображения под названием FidelityFX Super Resolution (FSR). Хоть и технологии 
работают по разным принципам, но задача стоит одна, улучшить изображение 
максимально эффективно, значит и сравнение технологий уместная задача.
Самая первая графика для компьютера была векторная. Простоя графика для таких 
систем не проблема, а вот, например, чтобы вывести текст, эта технология была 
неудобной и, по сути, не могла быть масштабируемой и для дальнейшего применения 
необходимо было переходить на строчное отображение данных. Где каждая строка 
делилась на участки, которые с развитием техники и появлением современных 
мониторов стали совсем условными, так как эти участки и строки, стали отдельными 
пикселями, причем каждый из которых управляется отдельно. Но здесь есть проблема 
(рисунок 1), и она заключается в том, что пиксели имеют определённый размер, то есть 
они не могут отобразить детали графики, которые мельче чем сами пиксели. Данная 
проблема появилась в первых мониторах и никуда не исчезла в современном мире [4, 5].
Для решения этой проблемы существует два способа:
Первый – заставить изображение выглядеть так, как будто оно может отображать 
объекты размером меньше, чем один пиксель. Второй – делать пиксели меньше, а их 
количество больше, за счет чего уменьшать дискретизацию отображаемых объектов.
Оба этих способа друг друга не исключают, так как имеют свои достоинства и 
недостатки. Если со вторым способом все понятно, то есть физическое ограничение 
пикселей и дороговизна изготовления таких мониторов. То с первым способом, 
изначально стояла такая задача: внутри компьютера создать изображение более 
высокого разрешения, например, (рисунок 2), четыре пикселя за место одного, затем 
усреднить цвета и яркость этих пикселей и отобразить их уже в реальном аппаратном 
пикселе. По итогу получим, что объект, не имеющий полутонов, в силу своего цвета, 
получает полутона, которые позволяют, отображать границы перехода между 
объектами и объектами меньше пикселя. Но основной недостаток в том, что это 
требует много вычислительных ресурсов. Поэтому подход изменился, дальше начали 
разрабатывать алгоритмы, которые на основе имеющегося изображения, то есть без 
увеличения визуального изображения, должны восстанавливать среднее положение 
объекта. Причем изображение разделено пиксельной сеткой. Таких алгоритмов очень 
много, но основа таких алгоритмов – это анализ соседних пикселей и создание 
центрального с учетом действия соседних. Если смотреть относительно сглаживания, 
то это очень достойно, но недостаток в том, что теряется детализация, так как в один 
пиксель замешиваются другие пиксели [6].
Если же, создавать изображение в высоком разрешении сразу, то тут появляются 
свои проблемы. Технологическая, поскольку экран монитора состоит миллиона 
простых пикселей, которые по отдельности управляются, более того, управление не 
дискретное, они имеют градацию 256, 512, 1024 включения субпикселей, каждого из 
цветового канала, каждого пикселя и это все это должны манятся сотни раз в секунду. 
Информационная, предположим, у нас имеется монитор с разрешением FullHD и с 8 
битным субпикселем с градацией 256, получим скорость: 
1920×1080×3×8/8=6220800×60=373 248 000 бит/с, то есть около 3-х гигабит в секунду, 
чтобы вывести кадр. Если же будет стоять задача вывести 4K с дискретизацией в 10 
бит, при этом необходимо 240 кадров в секунду, то получим следующую величину 
скорости передачи: 3840×2160×3×10×240=59 719 680 000 000 бит/с.
Указанные две проблемы, не такие и простые, как может показаться. Но они 
решаемы, например, порты, по которым происходит передача кадров в монитор, имеют 
специальные функции сжатия данных [7]. И последнее, чтобы вывести кадр высокого 
разрешения, необходим очень мощный графический ускоритель. А для того, чтобы 
нарисовать детализированное изображение видеокартам не хватает 
производительности. Все это привело к тому, что теперь не так важно уметь создавать 
виртуальные пиксели, которые будут хорошо сливаться в один реальный. Необходимо, 
чтобы на основе имеющихся пикселей можно было бы реализовать возможности 
отображений, чего-то на реально существующих пикселей, когда данных для этого 
недостаточно. 
Сначала появились технологии для сглаживания TXAA, FXAA, MSAA и так далее, они 
и послужили началом для создания технологии DLSS.
Технология DLSS. Перед тем как, использовать глубокую нейросеть, ее обучает 
следующим образом. Сначала нейросети дают изучать эталонные изображения в 
высоком разрешении, причем обязательно это качество 16K (рисунок 3). Эти 
изображения создаются мощным суперкомпьютером во время офлайн отрисовки 
приложения с низкой частотой смены кадров. Благодаря этому изучению нейросеть в 
дальнейшим на основе исходного изображения в низком качестве способна создавать 
кадры в высоком разрешении уже на компьютере пользователя. В этом она упирается 
на данные, полученные в процессе обучения, когда нейросеть сама создает кадры из 
образцов низкого разрешения, их сравнивают с эталонными в разрешении 16K и обо 
всех недочетах сообщают обратно нейросети и с каждым циклом она обучается и 
улучшает свои результаты. Конечные результаты попадают уже в драйвер для 
видеокарт [8].
Для дальнейшей работы, чтобы передавать кадры для отображения, необходимо 
получить входные данные. Первые данные это несколько кадров в низком разрешении 
без сглаживания, вторые это векторы движения для этих изображений. В данном 
случаи под вектором подразумевается, то в каком направлении со сменой каждого 
кадра движутся объекты на этом кадре. То есть нейросеть определяет, как попиксельно 
повысить разрешение следующего кадра. Все эти данные предоставляет игровой 
движок приложения. Далее происходит добавление остаточных эффектов, например, 
блюм – это эффект размытия света на ярких краях сцены. На рисунке 4 представлена 
схема этого процесса [9].
DLSS 1.0 – самая первая версия технологии, представленная компанией NVIDIA. 
Данная версия требовала обучение искусственного интеллекта под каждую новую игру, 
поддержку со стороны разработчика, то есть регулярные правки в код и специальных 
драйверов для видеокарты. Качество изображения с включенным DLSS 1.0, 
действительно было лучше и количество кадров было больше в среднем на 20-30%, но 
данная версия не справлялась с движущимся объектами, так как было заметно рябь за 
сместившимся объектом. Также были проблемы с мелкими объектами, они были 
размытыми.
DLSS 2.0 – в этой версии компания NVIDIA исправила недостатки предыдущего 
поколения. Не было ряби за сместившимся объектом, мелкие объекты стали четче, 
границы стали более резкими и прорисованными, а также не большое повышение 
производительности в целом. Этого удалось добиться тем, что модель искусственного 
интеллекта была доработана так, что стала эффективнее в 2 раза, предыдущей модели. 
При этом стала более эффективнее использовать тензорные ядра. Благодаря всему 
вышеперечисленному, технологию стало проще встраивать в приложения.
DLSS 3.0 – эту версию улучшили, добавив новую функцию под названием 
«оптическая мультикадровая генерация». Она позволяет создавать не обычные 
пиксели, а сразу целые новые кадры, что в свое очередь позволило снизить количество 
графических ошибок в кадре. Например, когда искусственному интеллекту нужно 
отрисовать освещение, мелкие частицы, отражения от объектов. Далее полученные 
кадры, уже передаются нейросети, которая их анализирует, и на основе их создает 
новые кадры, которые уже будут переданы на экран. Это все дает преимущество DLSS, 
в том, что она может создавать в среднем 7/8 от всех отображаемых пикселей с 
помощью искусственного интеллекста. По итогу получаем, что количество кадров 
увеличилось до 4 раз, по сравнению с тем, где не используется DLSS [10].
Технология FSR. Технология FSR разработана компанией AMD, в противовес 
технологии DLSS. Поскольку в сравнении капитала, компания AMD меньше, чем 
NVIDIA, то и разработать технологии необходимо было при помощи простого решения. 
И компания с этим справилась, выпустив FidelityFX Super Resolution. Главная 
особенность FSR в том, что эта технология не требует специальных аппаратных 
внедрений в графический адаптер, а также технология распространяется с открытым 
исходным кодом [11]. Когда происходит включение технологии FSR, то после этого 
отрисовка кадра происходит в меньшем разрешении, чем сам экран. Далее, происходит 
масштабирование кадра. Затем кадр получает постобработку, то есть это добавление, 
контраста, резкости граней, общей четкости. И по итогу, на этот кадр добавляются 
элементы интерфейса приложения в исходном разрешении. Схему процесса 
изображена на рисунке 5.
FSR 1.0 – самая первая версия технологии, представленная компанией AMD. Эта 
версия использует технологию пространственного масштабирования. Чтобы повысить 
производительность, она отрисовывает игровые визуальные эффекты с низким 
качеством, а после чего масштабирует изображение. Недостаток, этой версий в том, что 
при низких разрешениях, например 1080p, хорошо заметна визуальная неточность. При 
более высоких качествах разрешения, недостаток себя не проявляет.
FSR 2.0 – вторая версия технологии, где компания AMD внесла улучшения путем 
оптимизации процесса, а также в этой версии используется временное 
масштабирование. Оно получает данные о движении из нескольких кадров. Далее в 
процессе работы изображение перерабатывается, чтобы приблизиться на столько, на 
сколько это возможно к возможному качеству оригинала.
FSR 3.0 – объявлено, что новая версия находится в разработке, но никаких 
подробностей нет [12].
Сравнение технологии FSR и DLSS. Перед тем как начать сравнение необходимо 
описать испытательный стенд, на котором производились тесты: процессор – Intel core
7900x, ОЗУ – 16 Гб, видеокарта – RTX 3060. Сравнение будут происходить в 
следующих приложениях, это «Cyberpunk 2077», «Dying Light 2», «Forza Horizon 5». 
Данные приложения, были выбраны не случайно, а только потому, что в них 
поддерживаются обе технологии. Изображения тестов будут только по приложению 
«Cyberpunk 2077», так как, в остальных приложениях ситуация схожая. Версии 
технологии были выбраны следующие: FSR 2 и DLSS 2. Это сделано потому, что о FSR
3 пока еще ничего неизвестно, а DLSS 3 поддерживается только видеокартами с 
микроархитектурой Ada Lovelace. На данный момент бюджетных моделей нет. И 
получается, что пока есть основания сравнивать только вторые версии технологий [13].
На рисунках 6 и 7, показаны результаты тестирования DLSS в приложении 
«Cyberpunk 2077» в качестве 1440p или Quad HD. Тестирование технологии 
производилось в разных режимах работы:
Буква A соответствует «NATIVE1440p», что значит без использования технологии 
DLSS. Буква B соответствует «DLSS QUALITY», что значит максимально возможное 
качество кадра. Буква C соответствует «DLSS BALANCE», что значит баланс между 
производительностью и качеством кадра. Буква D соответствует «DLSS
PERFORMANCE», что значит более высокой производительности в ущерб качеству. 
Буква E соответствует «DLSS ULTRA PERFORMANCE», что значит максимально 
возможная производительность.
На рисунках 8 и 9, показаны результаты тестирования FSR в приложении «Cyberpunk
2077» в качестве Quad HD. Тестирование технологии производилось в таких же 
режимах работы, что и DLSS, соответственно.
Итог сравнения технологии по производительности примерно равный, поскольку 
прирост в процентной соотношений между FSR и DLSS приблизительно равен. А вот с 
качеством картинки, есть проблемы, во-первых, разницу достаточно тяжело заметить, 
во-вторых, все зависит от внимательности смотрящего. Но, тем не менее, у FSR в 
режиме «QUALITY» получается лучше отобразить растительность, но в тоже время, на 
контуре деревьев проявляются графические ошибки, например, мерцание. В тоже 
время, в режиме «BALANCE», обе технологии имеют графические ошибки на контурах 
удаленных объектов. К этому у FSR добавляется рябь по краям удаленных объектов. В 
режимах производительность обе технологии делают кадр слишком размытым, но это 
нормально, так как данные режимы предназначены для более высоких разрешений 
кадра [14].
На рисунках 10 и 11, показаны результаты тестирования DLSS в приложении 
«Cyberpunk 2077» в качестве 2160p или 4K. Тестирование технологии производилось 
также в разных режимах работы, как и до этого.
На рисунках 12 и 13, показаны результаты тестирования FSR в приложении 
«Cyberpunk 2077» в качестве 2160p. Тестирование технологии производилось в таких 
же режимах работы, что и DLSS, соответственно.
Итог сравнения технологии в 4K, по производительности, примерно равный, 
поскольку прирост в процентном соотношении между FSR и DLSS приблизительно 
равен. Качество кадра стало лучше, но FSR по-прежнему имеет мерцания во всех 
режимах.
При тестировании технологии в приложении «Dying Light 2», они показали себя 
достаточно уверено, и даже мерцаний у FSR не было замечено. Это, скорее всего, 
объясняется тем, что разработчики приложения оптимизировали работу технологий в 
своем продукте. А при тестировании этого же приложения в качестве 4K, заметно, что 
при DLSS, качество в целом лучше. Но и проблем с мерцанием у FSR не было.
При тестировании технологии в приложении «Forza Horizon 5» в качестве 4K
заметно, что кадр при включенной технологии DLSS более четкий, а при FSR более 
размытый. При этом обе технологии показали себя отлично.
Вывод. Обе технологии достаточно интересны, но и обе имеют свои недостатки и 
преимущества. DLSS работает только на видеоадаптерах начиная с микроархитектуры 
Turing, но при этом дают более качественную картинку. FSR же имеет не такую 
качественную картинку, но в тоже время поддерживаться может более слабыми 
видеоадаптерами. Стоит также обратить внимание, что из-за того, что компания 
NVIDIA, дополнительно устанавливает тензорные ядра в свои видеокарты, а это 
приводит к удорожанию адаптеров, в сравнении цена/производительность с адаптерами 
от компании AMD.