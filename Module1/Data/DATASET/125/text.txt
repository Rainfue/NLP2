Искусственные нейронные сети представляют собой математический 
аппарат для эффективной обработки входных сигналов, построенный по 
принципам организации и функционирования биологической высшей нервной 
деятельности. Они предназначены для решения широкого круга задач, таких, 
как распознавание образов, классификация, кластеризация, аппроксимация, 
прогнозирование временных рядов, адаптивное управления и множества 
других. Разработка программно-алгоритмического обеспечения на основе 
искусственных нейронных сетей – актуальное направление нейроинформатики, 
так как сами нейронные сети применяются в различных сферах деятельности: 
медицине, промышленности, экономике, машиностроении и других. 
Нейронные сети позволяют проводить качественное математическое 
моделирование свойств и ситуаций, обладают достаточной гибкостью и 
расширяемостью, а также позволяют экономить огромные средства на 
экспериментах.
Существующее множество нейросетевых парадигм с избытком 
перекрывает разнообразие классов задач, для решения которых они 
используются. Однако исследователи продолжают предлагать новые 
архитектурные решения и модификации известных архитектур нейронных 
сетей, что позволяет качественно улучшать получаемые с их помощью решения 
и ускорять этот процесс.
Данная работа посвящена модификации однослойных и многослойных 
перцептронов. Ее суть заключается в применении нелинейного взвешивания 
входных сигналов каждого нейрона вместо традиционно используемого 
линейного взвешивания. Основная цель такой модификации – структурное 
упрощение перцептрона при сохранении удовлетворительного уровня ошибки. 
Также для ряда задач появляется ранее отсутствовавшая возможность их 
решения с помощью перцептронов с меньшим количеством слоев, включая 
однослойные перцептроны.
В соответствии с поставленной целью было изменено правило 
комбинирования входящих сигналов в нейрон. Новое состояние каждого 
нейрона рассчитывается по соотношению:,
где i – индекс входного сигнала; j – индекс нейрона; m – количество 
входных сигналов нейрона; k – порядок одночлена в многочлене, описывающем 
состояние нейрона; K – порядок самого многочлена.
Таким образом, традиционное линейное взвешивание входных сигналов 
также удовлетворяет этому соотношению при K = 1. Нами для изучения были 
взяты перцептроны со вторым порядком состояния нейронов (K = 2). Это 
привело к тому, что каждая синаптическая связь, кроме смещения, стала 
описываться двумя коэффициентами: линейным и квадратичным.
Наиболее распространенный метод обучения многослойных перцептронов 
– метод обратного распространения ошибки (ОРО), который, однако, с учетом 
изменения порядка состояния нейронов в новой модификации перцептрона, 
также должен быть скорректирован. В этой связи в работе предложен 
двухстадийный алгоритм обучения многослойного перцептрона.
Идея данного алгоритма заключается в том, что на первой стадии весовые 
коэффициенты нейронной сети wij 0 и wij 1 настраиваются по методу ОРО до их 
стабилизации в пределах заданного уровня изменения [1, с. 42–43]. При этом 
весовые коэффициенты 2-го порядка остаются нулевыми (wij 2 = 0). На второй 
стадии квадратичные веса инициализируются небольшими по абсолютному 
значению случайными числами, после чего одновременно все весовые 
коэффициенты корректируются методом наискорейшего градиентного спуска 
(НГС), стремясь минимизировать функцию ошибки – квадрат разности 
экспериментального и расчетного выходных значений:
где q – индекс обучающего примера; Q – количество обучающих 
примеров; N – количество выходов перцептрона.
Наиболее ресурсоемкая стадия данного алгоритма – вторая, в ходе которой 
308
требуется численно рассчитать частные производные функции ошибки по 
каждому весовому коэффициенту, координаты направления наискорейшего 
уменьшения ошибки и, наконец, скорректировать веса.
В целях уменьшения объема вычислений и ускорения спуска к оптимуму в 
алгоритме использован переменный шаг поиска. Если новые весовые 
коэффициенты приводили к уменьшению функции ошибки, шаг поиска 
увеличивался вдвое. Если новые коэффициенты оказывались неудачными, шаг 
поиска, наоборот, уменьшался вчетверо, а поиск возобновлялся из последней 
лучшей точки.
Важно заметить, что в соответствии с основной идеей метода НГС 
пересчет направления поиска производился только тогда, когда текущее 
направление переставало давать лучшие весовые коэффициенты, что также 
способствовало уменьшению объема вычислений.
Для практического исследования предложенной модификации 
перцептрона и алгоритма его обучения была выбрана нейронная сеть с 3 
входами и 2 выходами. Для обучающей выборки первый выход рассчитывался 
как. Все входные 
переменные принадлежали пределам нормализации [0, 1]. Область допустимых 
значений выходных переменных удовлетворяла тому же диапазону. Для 
обучения использовались структуры двухслойного перцептрона с 2, 3 и 5 
скрытыми нейронами. Функция активации – сигмоидная логистическая с 
параметром насыщения 1,0. Объем обучающей выборки – 150 примеров.
В табл. 1 для сравнения показаны результаты обучения перцептронов 
разной структуры с традиционным линейным и предложенным квадратичным 
взвешиванием входных переменных. E1 и E2 – ошибки соответствующих 
выходов сети, E – средняя ошибка. Обучение стандартным методом ОРО 
продолжалось 10000 эпох. Обучение по предложенному двухстадийному 
алгоритму продолжалось 5000 эпох методом ОРО и 5000 шагов методом НГС.
Таблица 1. Результаты обучения перцептрона
Нейронов 
в скрытом 
слое
Метод обработки входных 
сигналов
E1 E2 E
Время 
обучения, 
с
2
Линейный (метод ОРО) 0,177 0,059 0,132 47
Квадратичный (методы ОРО 
и НГС)
0,081 0,037 0,063 290
3
Линейный (метод ОРО) 0,044 0,038 0,041 53
Квадратичный (методы ОРО 
и НГС)
0,040 0,030 0,035 452
5
Линейный (метод ОРО) 0,027 0,015 0,022 65
Квадратичный (методы ОРО 
и НГС)
0,026 0,014 0,021 1087
Из приведенных данных следует, что предложенный метод нелинейной 
обработки входных сигналов оказывается наиболее эффективным для самых 
простых структур перцептронов (2 скрытых нейрона). Положительный эффект 
от двукратного снижения уровня ошибки полностью компенсирует такой 
недостаток, как увеличение времени обучения. Для более сложных структур 
перцептронов представляется целесообразным в дальнейшем модифицировать 
или найти альтернативный алгоритм обучения, так как предложенный 
двухстадийный алгоритм, по всей видимости, останавливается в локальном 
оптимуме ошибки. Одним из вариантов преодоления локального минимума 
может быть использование метода моментов [2, с. 1].
Еще одной задачей дальнейших исследований является адаптация 
алгоритма обратного распространения ошибки к нелинейному взвешиванию 
входов в нейронах перцептрона, что должно значительно ускорить процесс 
обучения рассмотренной модификации нейронной сети.
