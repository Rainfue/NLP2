Каждый процесс построения нейронной сети — это комплексная задача, 
требующая глубоких навыков анализа данных, высшей математики, программирования и машинного обучения. Библиотека TensorFlow упростила 
разработку отдельных задач и подход к созданию нейронных сетей, но не 
свела их к минимуму. 
Анализ, предобработка, визуализация данных и моделирование нейронных сетей выполнены на языке программирования Python с использованием 
следующих основных библиотек:
- NumPy - библиотека, предоставляющая методы линейной алгебры 
для манипуляции с большими массивами и матрицами;
- Pandas - библиотека для чтения выборки данных и ее анализа;
- TensorFlow (TF) библиотека, содержащая методы глубокого 
обучения, включая функции реализации нейронных сетей;
- Matplotlib и Seaborn - библиотеки для визуализации данных.
Рекуррентная нейронная сеть LSTM благодаря обратным связям и 
долгой краткосрочной памяти позволяет обрабатывать текст с большим 
количеством символов, в данном случае HTTP запрос, и осуществлять поиск 
атак, но для этого ей нужны классификаторы всех видов атак. Получается, что 
в чистом виде данную архитектуру применить нельзя, необходимо перейти к 
архитектуре позволяющей применять обучение без учителя. Такой 
архитектурой является автокодировщик.
Существует много способов реализовать модель Sequence to Sequence. 
Один из них — это разбить модель на две подмодели: кодер и декодер. Первая 
подмодель принимает необработанные входные текстовые данные. 
Результатом её работы является нейронный образ в виде целевого вектора. 
Вторая подмодель восстанавливает входные текстовые данные, на основе 
целевого вектора и вычисляет степень ошибки.
Декодер использует два разных типа ввода: для обучения и для 
восстановления. При работе в режиме восстановления выходные данные 
каждого временного шага являются входными данными для следующего 
временного шага. При работе декодера в режиме обучения входные данные 
предоставляются как целевые, но с добавлением специального токена.
В основе кодера и декодера лежит рекуррентная нейросеть LSTM. Для 
того чтобы эта сеть функционировала, она имеет два скрытых слоя. При этом 
стоит учитывать, что на вход скрытых слоёв поступают обработанные данные 
в виде векторов. Для такой обработки необходим слой конвертации 
«embending». Для расчёта, хранения и анализа вероятностей полученных на 
выходе скрытых слоёв декодера используется проекционный слой. Соединив 
эти слои, получается модель Sequence to Sequence, включающая кодер и 
декодер.
Модель кодера состоит из двух разных частей. Первая часть - это слой 
«embending». Каждый символ в пакете будет представлен как вектор 
(encoding_embedding_size). Вторая часть — это слой LSTM. В данной модели 
несколько ячеек LSTM складываются вместе после применения функции 
«dropout». Данная функция фильтрует входной слой, удаляя некоторые 
элементы, для облегчения последующих вычислений. У данной функции есть 
единственный параметр, вероятность того, что каждый элемент сохранен. 
Рассмотрим модули, используемые для создания кодера.
Модуль TF «contrib.layers.embed_sequence» используется для создания 
слоя «embending».
Модель декодера можно рассматривать как два отдельных процесса: 
обучение и восстановление. Эти процессы реализуются путём смены 
алгоритма смещения декодера. Во время обучения вероятность появления 
символа равна 1. При процессе восстановления учитывается тот символ, у 
которого вероятность максимальная.
Хотя кодировщик использует «TF contrib.layers.embed_sequence» , он не 
применим к декодеру, даже если использовать входные данные в виде вектора. 
Это связано с тем, что один и тот же «embending» вектор должен совместно 
использоваться на этапах обучения и восстановления. 
TF «contrib.layers.embed_sequence» может конвертировать 
подготовленный набор данных в векторный вид перед запуском НС, что 
необходимо для процесса восстановления. При этом невозможно внедрить 
выходные данные процесса вывода до запуска модели, потому что выходные 
данные текущего временного шага являются входными данными следующего 
временного шага. Это решается путём использования динамического 
конвертирования.
Хоть и входные данные поступают из одного слоя, они обрабатываются 
разными частями слоя «embending».
Для получения результатов работы декодера необходимо создать 
выходной слой. Выходной слой — это анализ данных, полученных в 
проекционном слое. По сути, это просто вывод массива [символ, 
максимальная вероятность] для каждого шага.
Последний же шаг в создании модели Sequence to Sequence — это 
решение и применение алгоритмов оптимизации.
Градиентное отсечение — это метод, при котором производные ошибки 
изменяются или обрезаются до порогового значения во время обратного 
распространения по сети и с использованием отсеченных градиентов для 
обновления весов.
Для реализации градиентной обрезки необходимо установить пороговые 
значения, чтобы градиент находился в определенной границе.
В качестве объективной метрики качества построенной модели на тестовой выборке используется метрика «balanced accuracy score, сбалансированная оценка доли правильных ответов», вычитывающаяся по формуле:
По результатам тестирования что данная модель обладает высокой 
точностью определения вредоносных HTTP запросов, но имеет значительные 
недостатки:
- Модель очень долго обучается. Если обучающая база данных из 
20000 запросов, то обучение модели длятся 289 мин (примерно 4,5 
часа). 
- Среднее время обработки одного запроса составляет 1.2 мс.
Данные недостатки можно компенсировать с помощью использования 
высокопроизводительного оборудования.
Таким образом была смоделирована и оптимизирована нейронная сеть с 
архитектурой Sequence to Sequence, позволяющая эффективно анализировать 
HTTP запросы